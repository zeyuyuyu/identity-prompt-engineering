"""
Identity Prompt Engineering å®éªŒä¸»è„šæœ¬
æ¢ç´¢ System Prompt ä¸­èº«ä»½å˜åŒ–å¯¹ LLM è¾“å‡ºçš„å½±å“
"""

import os
import json
import time
from datetime import datetime
from typing import Dict, List, Optional
from openai import OpenAI
from config import IDENTITIES, TEST_QUESTIONS, EXPERIMENT_PARAMS, OPENAI_MODEL

# åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
client = OpenAI()

def get_response(
    identity_key: str,
    question: str,
    temperature: float = 0.7,
    max_tokens: int = 1000
) -> Dict:
    """
    ä½¿ç”¨æŒ‡å®šèº«ä»½è·å– LLM å“åº”
    """
    identity = IDENTITIES[identity_key]
    system_prompt = identity["system_prompt"]
    
    start_time = time.time()
    
    try:
        response = client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": question}
            ],
            temperature=temperature,
            max_tokens=max_tokens
        )
        
        end_time = time.time()
        
        return {
            "success": True,
            "response": response.choices[0].message.content,
            "model": response.model,
            "usage": {
                "prompt_tokens": response.usage.prompt_tokens,
                "completion_tokens": response.usage.completion_tokens,
                "total_tokens": response.usage.total_tokens
            },
            "latency": end_time - start_time
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "response": None
        }

def run_single_experiment(
    identity_key: str,
    question_data: Dict,
    run_id: int = 1
) -> Dict:
    """
    è¿è¡Œå•æ¬¡å®éªŒ
    """
    result = get_response(
        identity_key=identity_key,
        question=question_data["question"],
        temperature=EXPERIMENT_PARAMS["temperature"],
        max_tokens=EXPERIMENT_PARAMS["max_tokens"]
    )
    
    return {
        "identity_key": identity_key,
        "identity_name": IDENTITIES[identity_key]["name"],
        "question_id": question_data["id"],
        "question": question_data["question"],
        "category": question_data["category"],
        "difficulty": question_data["difficulty"],
        "run_id": run_id,
        "timestamp": datetime.now().isoformat(),
        **result
    }

def run_full_experiment(
    identities: List[str] = None,
    categories: List[str] = None,
    num_runs: int = 1,
    output_file: str = "results.json"
) -> List[Dict]:
    """
    è¿è¡Œå®Œæ•´å®éªŒ
    
    Args:
        identities: è¦æµ‹è¯•çš„èº«ä»½åˆ—è¡¨ï¼ŒNone è¡¨ç¤ºå…¨éƒ¨
        categories: è¦æµ‹è¯•çš„é—®é¢˜ç±»åˆ«ï¼ŒNone è¡¨ç¤ºå…¨éƒ¨
        num_runs: æ¯ä¸ªç»„åˆè¿è¡Œæ¬¡æ•°
        output_file: ç»“æœè¾“å‡ºæ–‡ä»¶
    """
    if identities is None:
        identities = list(IDENTITIES.keys())
    
    if categories is None:
        categories = list(TEST_QUESTIONS.keys())
    
    results = []
    total_combinations = 0
    
    # è®¡ç®—æ€»ç»„åˆæ•°
    for category in categories:
        total_combinations += len(TEST_QUESTIONS[category]) * len(identities) * num_runs
    
    print(f"=" * 60)
    print(f"Identity Prompt Engineering å®éªŒ")
    print(f"=" * 60)
    print(f"æ¨¡å‹: {OPENAI_MODEL}")
    print(f"èº«ä»½æ•°é‡: {len(identities)}")
    print(f"é—®é¢˜ç±»åˆ«: {categories}")
    print(f"æ¯ç»„åˆè¿è¡Œæ¬¡æ•°: {num_runs}")
    print(f"æ€»å®éªŒæ•°: {total_combinations}")
    print(f"=" * 60)
    
    current = 0
    
    for category in categories:
        print(f"\nğŸ“ ç±»åˆ«: {category}")
        
        for question_data in TEST_QUESTIONS[category]:
            print(f"\n  â“ é—®é¢˜: {question_data['id']}")
            
            for identity_key in identities:
                identity_name = IDENTITIES[identity_key]["name"]
                
                for run in range(1, num_runs + 1):
                    current += 1
                    print(f"    [{current}/{total_combinations}] èº«ä»½: {identity_name}, è¿è¡Œ #{run}...", end=" ")
                    
                    result = run_single_experiment(
                        identity_key=identity_key,
                        question_data=question_data,
                        run_id=run
                    )
                    results.append(result)
                    
                    if result["success"]:
                        print(f"âœ“ ({result['latency']:.2f}s, {result['usage']['total_tokens']} tokens)")
                    else:
                        print(f"âœ— Error: {result.get('error', 'Unknown')}")
                    
                    # é¿å… rate limiting
                    time.sleep(0.5)
    
    # ä¿å­˜ç»“æœ
    output_path = os.path.join(os.path.dirname(__file__), output_file)
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\n{'=' * 60}")
    print(f"âœ… å®éªŒå®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    print(f"æˆåŠŸ: {sum(1 for r in results if r['success'])}/{len(results)}")
    print(f"{'=' * 60}")
    
    return results

def run_quick_demo():
    """
    å¿«é€Ÿæ¼”ç¤ºï¼šç”¨å°‘é‡èº«ä»½å’Œé—®é¢˜æµ‹è¯•
    """
    print("\nğŸš€ å¿«é€Ÿæ¼”ç¤ºæ¨¡å¼")
    print("é€‰æ‹© 3 ä¸ªèº«ä»½å’Œ 2 ä¸ªé—®é¢˜è¿›è¡Œæµ‹è¯•\n")
    
    demo_identities = ["none", "doctor", "lawyer"]
    demo_categories = ["medical", "legal"]
    
    return run_full_experiment(
        identities=demo_identities,
        categories=demo_categories,
        num_runs=1,
        output_file="demo_results.json"
    )

def run_specific_test(identity_key: str, question: str):
    """
    å•ç‹¬æµ‹è¯•ç‰¹å®šèº«ä»½å’Œé—®é¢˜
    """
    print(f"\nğŸ”¬ å•ç‹¬æµ‹è¯•")
    print(f"èº«ä»½: {IDENTITIES[identity_key]['name']}")
    print(f"é—®é¢˜: {question[:50]}...")
    print("-" * 40)
    
    result = get_response(identity_key, question)
    
    if result["success"]:
        print(f"\nğŸ“ å›ç­”:\n{result['response']}")
        print(f"\nğŸ“Š ç»Ÿè®¡: {result['usage']['total_tokens']} tokens, {result['latency']:.2f}s")
    else:
        print(f"âŒ é”™è¯¯: {result['error']}")
    
    return result

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Identity Prompt Engineering å®éªŒ")
    parser.add_argument("--mode", choices=["demo", "full", "test"], default="demo",
                       help="è¿è¡Œæ¨¡å¼: demo(å¿«é€Ÿæ¼”ç¤º), full(å®Œæ•´å®éªŒ), test(å•ç‹¬æµ‹è¯•)")
    parser.add_argument("--identity", type=str, help="æµ‹è¯•ç‰¹å®šèº«ä»½ (testæ¨¡å¼)")
    parser.add_argument("--question", type=str, help="æµ‹è¯•ç‰¹å®šé—®é¢˜ (testæ¨¡å¼)")
    parser.add_argument("--runs", type=int, default=1, help="æ¯ç»„åˆè¿è¡Œæ¬¡æ•°")
    
    args = parser.parse_args()
    
    if args.mode == "demo":
        run_quick_demo()
    elif args.mode == "full":
        run_full_experiment(num_runs=args.runs)
    elif args.mode == "test":
        if args.identity and args.question:
            run_specific_test(args.identity, args.question)
        else:
            print("test æ¨¡å¼éœ€è¦ --identity å’Œ --question å‚æ•°")
            print(f"å¯ç”¨èº«ä»½: {list(IDENTITIES.keys())}")

