"""
å®éªŒç»“æœåˆ†æè„šæœ¬
å®šé‡åˆ†æ + å®šæ€§åˆ†æ
"""

import json
import os
from collections import defaultdict
from typing import Dict, List
import statistics

def load_results(file_path: str = "results.json") -> List[Dict]:
    """åŠ è½½å®éªŒç»“æœ"""
    full_path = os.path.join(os.path.dirname(__file__), file_path)
    with open(full_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def quantitative_analysis(results: List[Dict]) -> Dict:
    """
    å®šé‡åˆ†æ
    """
    analysis = {
        "summary": {},
        "by_identity": defaultdict(lambda: {"responses": [], "latencies": [], "tokens": []}),
        "by_category": defaultdict(lambda: {"responses": [], "latencies": [], "tokens": []}),
        "by_identity_category": defaultdict(lambda: defaultdict(list)),
        "response_lengths": defaultdict(list)
    }
    
    successful_results = [r for r in results if r.get("success", False)]
    
    # åŸºç¡€ç»Ÿè®¡
    analysis["summary"]["total_experiments"] = len(results)
    analysis["summary"]["successful"] = len(successful_results)
    analysis["summary"]["failed"] = len(results) - len(successful_results)
    analysis["summary"]["success_rate"] = len(successful_results) / len(results) if results else 0
    
    for result in successful_results:
        identity = result["identity_name"]
        category = result["category"]
        response = result.get("response", "")
        latency = result.get("latency", 0)
        tokens = result.get("usage", {}).get("total_tokens", 0)
        
        # æŒ‰èº«ä»½ç»Ÿè®¡
        analysis["by_identity"][identity]["responses"].append(response)
        analysis["by_identity"][identity]["latencies"].append(latency)
        analysis["by_identity"][identity]["tokens"].append(tokens)
        
        # æŒ‰ç±»åˆ«ç»Ÿè®¡
        analysis["by_category"][category]["responses"].append(response)
        analysis["by_category"][category]["latencies"].append(latency)
        analysis["by_category"][category]["tokens"].append(tokens)
        
        # å“åº”é•¿åº¦
        analysis["response_lengths"][identity].append(len(response))
        
        # èº«ä»½xç±»åˆ«çŸ©é˜µ
        analysis["by_identity_category"][identity][category].append({
            "question_id": result["question_id"],
            "response_length": len(response),
            "tokens": tokens,
            "latency": latency
        })
    
    return analysis

def print_quantitative_report(analysis: Dict):
    """
    æ‰“å°å®šé‡åˆ†ææŠ¥å‘Š
    """
    print("\n" + "=" * 70)
    print("ğŸ“Š å®šé‡åˆ†ææŠ¥å‘Š")
    print("=" * 70)
    
    # æ€»ä½“ç»Ÿè®¡
    summary = analysis["summary"]
    print(f"\nğŸ“ˆ æ€»ä½“ç»Ÿè®¡:")
    print(f"  æ€»å®éªŒæ•°: {summary['total_experiments']}")
    print(f"  æˆåŠŸ: {summary['successful']} ({summary['success_rate']*100:.1f}%)")
    print(f"  å¤±è´¥: {summary['failed']}")
    
    # æŒ‰èº«ä»½ç»Ÿè®¡
    print(f"\nğŸ‘¤ æŒ‰èº«ä»½ç»Ÿè®¡:")
    print("-" * 70)
    print(f"{'èº«ä»½':<12} {'å¹³å‡å“åº”é•¿åº¦':<15} {'å¹³å‡å»¶è¿Ÿ(s)':<15} {'å¹³å‡Tokenæ•°':<15}")
    print("-" * 70)
    
    for identity, data in analysis["by_identity"].items():
        avg_length = statistics.mean([len(r) for r in data["responses"]]) if data["responses"] else 0
        avg_latency = statistics.mean(data["latencies"]) if data["latencies"] else 0
        avg_tokens = statistics.mean(data["tokens"]) if data["tokens"] else 0
        print(f"{identity:<12} {avg_length:<15.0f} {avg_latency:<15.2f} {avg_tokens:<15.0f}")
    
    # æŒ‰é—®é¢˜ç±»åˆ«ç»Ÿè®¡
    print(f"\nğŸ“ æŒ‰é—®é¢˜ç±»åˆ«ç»Ÿè®¡:")
    print("-" * 70)
    print(f"{'ç±»åˆ«':<15} {'å¹³å‡å“åº”é•¿åº¦':<15} {'å¹³å‡å»¶è¿Ÿ(s)':<15} {'å¹³å‡Tokenæ•°':<15}")
    print("-" * 70)
    
    for category, data in analysis["by_category"].items():
        avg_length = statistics.mean([len(r) for r in data["responses"]]) if data["responses"] else 0
        avg_latency = statistics.mean(data["latencies"]) if data["latencies"] else 0
        avg_tokens = statistics.mean(data["tokens"]) if data["tokens"] else 0
        print(f"{category:<15} {avg_length:<15.0f} {avg_latency:<15.2f} {avg_tokens:<15.0f}")

def qualitative_analysis(results: List[Dict], output_file: str = "qualitative_report.md"):
    """
    å®šæ€§åˆ†æ - ç”Ÿæˆè¯¦ç»†çš„å¯¹æ¯”æŠ¥å‘Š
    """
    # æŒ‰é—®é¢˜IDåˆ†ç»„
    by_question = defaultdict(list)
    for r in results:
        if r.get("success"):
            by_question[r["question_id"]].append(r)
    
    report_lines = [
        "# Identity Prompt Engineering å®šæ€§åˆ†ææŠ¥å‘Š\n",
        f"ç”Ÿæˆæ—¶é—´: {__import__('datetime').datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n",
        "---\n\n"
    ]
    
    for question_id, responses in by_question.items():
        if not responses:
            continue
            
        question = responses[0]["question"]
        category = responses[0]["category"]
        
        report_lines.append(f"## é—®é¢˜: {question_id}\n\n")
        report_lines.append(f"**ç±»åˆ«:** {category}\n\n")
        report_lines.append(f"**é—®é¢˜å†…å®¹:**\n> {question}\n\n")
        report_lines.append("---\n\n")
        
        for resp in responses:
            identity = resp["identity_name"]
            response_text = resp.get("response", "N/A")
            tokens = resp.get("usage", {}).get("total_tokens", "N/A")
            
            report_lines.append(f"### èº«ä»½: {identity}\n\n")
            report_lines.append(f"**Tokenæ•°:** {tokens}\n\n")
            report_lines.append(f"**å›ç­”:**\n\n{response_text}\n\n")
            report_lines.append("---\n\n")
        
        report_lines.append("\n\n")
    
    # ä¿å­˜æŠ¥å‘Š
    output_path = os.path.join(os.path.dirname(__file__), output_file)
    with open(output_path, 'w', encoding='utf-8') as f:
        f.writelines(report_lines)
    
    print(f"\nğŸ“ å®šæ€§åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
    return output_path

def compare_responses(results: List[Dict], question_id: str):
    """
    å¯¹æ¯”ç‰¹å®šé—®é¢˜åœ¨ä¸åŒèº«ä»½ä¸‹çš„å›ç­”
    """
    print(f"\nğŸ” é—®é¢˜å¯¹æ¯”: {question_id}")
    print("=" * 70)
    
    question_results = [r for r in results if r["question_id"] == question_id and r.get("success")]
    
    if not question_results:
        print("æœªæ‰¾åˆ°è¯¥é—®é¢˜çš„ç»“æœ")
        return
    
    print(f"é—®é¢˜: {question_results[0]['question']}\n")
    
    for r in question_results:
        print(f"\n{'â”€' * 70}")
        print(f"ğŸ‘¤ èº«ä»½: {r['identity_name']}")
        print(f"ğŸ“Š Token: {r.get('usage', {}).get('total_tokens', 'N/A')}")
        print(f"{'â”€' * 70}")
        response = r.get("response", "N/A")
        # æ˜¾ç¤ºå‰500å­—ç¬¦
        if len(response) > 500:
            print(f"{response[:500]}...\n[æˆªæ–­ï¼Œå…±{len(response)}å­—ç¬¦]")
        else:
            print(response)

def find_interesting_differences(results: List[Dict]) -> List[Dict]:
    """
    æ‰¾å‡ºæœ‰è¶£çš„å·®å¼‚ - åŒä¸€é—®é¢˜ä¸‹å“åº”å·®å¼‚æœ€å¤§çš„æƒ…å†µ
    """
    by_question = defaultdict(list)
    for r in results:
        if r.get("success"):
            by_question[r["question_id"]].append(r)
    
    differences = []
    
    for question_id, responses in by_question.items():
        if len(responses) < 2:
            continue
        
        lengths = [len(r.get("response", "")) for r in responses]
        length_variance = statistics.variance(lengths) if len(lengths) > 1 else 0
        
        differences.append({
            "question_id": question_id,
            "question": responses[0]["question"],
            "category": responses[0]["category"],
            "variance": length_variance,
            "min_length": min(lengths),
            "max_length": max(lengths),
            "num_responses": len(responses)
        })
    
    # æŒ‰å·®å¼‚æ’åº
    differences.sort(key=lambda x: x["variance"], reverse=True)
    
    return differences

def print_interesting_differences(results: List[Dict], top_n: int = 5):
    """
    æ‰“å°æœ€æœ‰è¶£çš„å·®å¼‚
    """
    differences = find_interesting_differences(results)
    
    print(f"\nğŸ”¥ å“åº”å·®å¼‚æœ€å¤§çš„ Top {top_n} é—®é¢˜:")
    print("=" * 70)
    
    for i, diff in enumerate(differences[:top_n], 1):
        print(f"\n{i}. {diff['question_id']} (ç±»åˆ«: {diff['category']})")
        print(f"   é—®é¢˜: {diff['question'][:60]}...")
        print(f"   å“åº”é•¿åº¦èŒƒå›´: {diff['min_length']} - {diff['max_length']} å­—ç¬¦")
        print(f"   å·®å¼‚åº¦: {diff['variance']:.0f}")

def generate_full_report(results_file: str = "results.json"):
    """
    ç”Ÿæˆå®Œæ•´åˆ†ææŠ¥å‘Š
    """
    results = load_results(results_file)
    
    print("\n" + "ğŸ”¬ " * 20)
    print("       Identity Prompt Engineering å®éªŒåˆ†æ")
    print("ğŸ”¬ " * 20)
    
    # å®šé‡åˆ†æ
    analysis = quantitative_analysis(results)
    print_quantitative_report(analysis)
    
    # æœ‰è¶£å·®å¼‚
    print_interesting_differences(results)
    
    # ç”Ÿæˆå®šæ€§æŠ¥å‘Š
    qualitative_analysis(results)
    
    print("\n" + "=" * 70)
    print("âœ… åˆ†æå®Œæˆï¼")
    print("=" * 70)

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        results_file = sys.argv[1]
    else:
        # é»˜è®¤å°è¯•åŠ è½½ demo ç»“æœæˆ–å®Œæ•´ç»“æœ
        if os.path.exists(os.path.join(os.path.dirname(__file__), "demo_results.json")):
            results_file = "demo_results.json"
        else:
            results_file = "results.json"
    
    generate_full_report(results_file)

